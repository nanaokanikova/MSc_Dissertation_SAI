{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72910643-9b1d-4bcd-bc0c-d76ebe9d70f4",
   "metadata": {},
   "source": [
    "# Accessing model data from CEDA archives \n",
    "\n",
    "This notebook accesses precipitation and temperature data from first ensemble member for a number of CMIP6 models from the CEDA archives and stores them locally as nc files to be accessed by other notebooks for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea6dc76-d125-4668-899a-002d133f4816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/nanaokan/.conda/envs/cmipv2/lib/python3.13/site-packages/pyproj/network.py:59: UserWarning: pyproj unable to set PROJ database path.\n",
      "  _set_context_ca_bundle_path(ca_bundle_path)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cftime\n",
    "from xmip.preprocessing import rename_cmip6\n",
    "import os\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e252c725-a43a-4fdb-897f-230598d286ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1209/2496359610.py:142: SerializationWarning: Unable to decode time axis into full numpy.datetime64[ns] objects, continuing using cftime.datetime objects instead, reason: dates out of range. To silence this warning use a coarser resolution 'time_unit' or specify 'use_cftime=True'.\n",
      "  ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
      "/tmp/ipykernel_1209/2496359610.py:142: SerializationWarning: Unable to decode time axis into full numpy.datetime64[ns] objects, continuing using cftime.datetime objects instead, reason: dates out of range. To silence this warning use a coarser resolution 'time_unit' or specify 'use_cftime=True'.\n",
      "  ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
      "/tmp/ipykernel_1209/2496359610.py:142: SerializationWarning: Unable to decode time axis into full numpy.datetime64[ns] objects, continuing using cftime.datetime objects instead, reason: dates out of range. To silence this warning use a coarser resolution 'time_unit' or specify 'use_cftime=True'.\n",
      "  ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
      "/tmp/ipykernel_1209/2496359610.py:142: SerializationWarning: Unable to decode time axis into full numpy.datetime64[ns] objects, continuing using cftime.datetime objects instead, reason: dates out of range. To silence this warning use a coarser resolution 'time_unit' or specify 'use_cftime=True'.\n",
      "  ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
      "/tmp/ipykernel_1209/2496359610.py:142: SerializationWarning: Unable to decode time axis into full numpy.datetime64[ns] objects, continuing using cftime.datetime objects instead, reason: dates out of range. To silence this warning use a coarser resolution 'time_unit' or specify 'use_cftime=True'.\n",
      "  ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
      "/home/users/nanaokan/.conda/envs/cmipv2/lib/python3.13/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'pr' has multiple fill values {np.float32(1e+20), np.float64(1e+20)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/home/users/nanaokan/.conda/envs/cmipv2/lib/python3.13/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'pr' has multiple fill values {np.float32(1e+20), np.float64(1e+20)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/home/users/nanaokan/.conda/envs/cmipv2/lib/python3.13/site-packages/xarray/conventions.py:204: SerializationWarning: variable 'pr' has multiple fill values {np.float32(1e+20), np.float64(1e+20)} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndata_list = [patterns]\\nnames = [\\'patterns\\']\\n\\noutput_dir = \"inter_files\"\\nos.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn\\'t exist\\n\\n# Save each DataArray in the list\\nfor data in range(len(data_list)):\\n    dirpath = os.path.join(output_dir, names[data])\\n    os.makedirs(dirpath, exist_ok=True)\\n    for i in range(len(data_list[data])):\\n        filename = f\"{names[data]}_{i+1}.nc\"\\n        filepath = os.path.join(dirpath, filename)\\n\\n        # Save the DataArray to NetCDF format\\n        data_list[data][i].to_netcdf(filepath)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# PRECIP\n",
    "\n",
    "\n",
    "#list centres - precip\n",
    "cents_list_s2 = os.listdir('/badc/cmip6/data/CMIP6/ScenarioMIP/')\n",
    "cents_list_s2.remove('DWD') #doesnt have ssp245\n",
    "cents_list_s2.remove('HAMMOZ-Consortium') #doesnt have ssp245\n",
    "cents_list_s2.remove('IPSL') # doesn't have Amon\n",
    "cents_list_s2.remove('UA') # dimensions are weird \n",
    "cents_list_s2.remove('CNRM-CERFACS') # s245 only runs until 2020\n",
    "#cents_list_s2.remove('NASA-GISS') # only has tas no precip\n",
    "cents_list_s2.remove('MOHC')#cause hardcoding these two for correct model\n",
    "cents_list_s2.remove('NCAR') #cause hardcoding these two for correct model\n",
    "cents_list_s2.remove('THU') # values are 1000x smaller than others\n",
    "cents_list_s2.remove('EC-Earth-Consortium') # starts 1975: no PI\n",
    "\n",
    "#list centres \n",
    "cents_list_hist = os.listdir('/badc/cmip6/data/CMIP6/CMIP/')\n",
    "cents_list_hist.remove('UA') # dimensions are weird\n",
    "cents_list_hist.remove('IPSL') # doesn't have Amon\n",
    "cents_list_hist.remove('CNRM-CERFACS') # s245 only runs until 2020\n",
    "cents_list_hist.remove('MOHC')#cause hardcoding these two for correct model\n",
    "cents_list_hist.remove('NCAR') #cause hardcoding these two for correct model\n",
    "\n",
    "\n",
    "#list models(?)\n",
    "mods_list_s2 = []\n",
    "for c in cents_list_s2:\n",
    "    mods = os.listdir(f'/badc/cmip6/data/CMIP6/ScenarioMIP/{c}/')\n",
    "    if len(mods) >= 1:\n",
    "        mods_list_s2.append(mods[0])\n",
    "    elif len(mods) < 1:\n",
    "        print('THIS DOES NOT HAVE MODELS')\n",
    "\n",
    "#add the missing ones - surpassing loop:\n",
    "#cents_list_s2.append('UA')\n",
    "#mods_list_s2.append('IPSL-CM6A-LR')\n",
    "\n",
    "#list models(?)\n",
    "mods_list_hist = []\n",
    "for c in cents_list_hist:\n",
    "    mods = os.listdir(f'/badc/cmip6/data/CMIP6/CMIP/{c}/')\n",
    "    if len(mods) >= 1:\n",
    "        mods_list_hist.append(mods[0])\n",
    "    elif len(mods) < 1:\n",
    "        print('THIS DOES NOT HAVE MODELS')\n",
    "\n",
    "\n",
    "mods_remove = []\n",
    "cents_remove = []\n",
    "\n",
    "for x in range(len(mods_list_s2)):\n",
    "    if not mods_list_s2[x] in mods_list_hist:\n",
    "        mods_remove.append(mods_list_s2[x])\n",
    "        cents_remove.append(cents_list_s2[x])\n",
    "for x in range(len(cents_remove)):\n",
    "    cents_list_s2.remove(cents_remove[x])\n",
    "    mods_list_s2.remove(mods_remove[x])\n",
    "    \n",
    "mods_remove = []\n",
    "cents_remove = []\n",
    "for x in range(len(mods_list_hist)):\n",
    "    if not mods_list_hist[x] in mods_list_s2:\n",
    "        mods_remove.append(mods_list_hist[x])\n",
    "        cents_remove.append(cents_list_hist[x])\n",
    "for x in range(len(cents_remove)):\n",
    "    cents_list_hist.remove(cents_remove[x])\n",
    "    mods_list_hist.remove(mods_remove[x])\n",
    "\n",
    "mods_list = []\n",
    "cents_list = []\n",
    "for x in range(len(mods_list_s2)):\n",
    "    if mods_list_s2[x] in mods_list_hist:\n",
    "        mods_list.append(mods_list_s2[x])\n",
    "        cents_list.append(cents_list_s2[x])\n",
    "\n",
    "#list ensemble members\n",
    "'''\n",
    "\n",
    "ens_list_s2 = []\n",
    "\n",
    "if len(cents_list) == len(mods_list):\n",
    "    for i in range(len(cents_list)):\n",
    "        ens = os.listdir(f'/badc/cmip6/data/CMIP6/ScenarioMIP/{cents_list[i]}/{mods_list[i]}/ssp245/')\n",
    "        if len(ens) >= 1:\n",
    "            for e in range(len(ens)):\n",
    "                if not '.' in ens[e]:\n",
    "                    ens_list_s2.append(ens[e])\n",
    "                    break\n",
    "        elif len(ens) < 1:\n",
    "            print('THIS DOES NOT HAVE ENSEMBLE MEMBERS')\n",
    "else: print('CENTRES AND MODELS NOT SAME LENGTH')\n",
    "\n",
    "'''\n",
    "\n",
    "ens_list_s2 = []\n",
    "\n",
    "if len(cents_list) == len(mods_list):\n",
    "    for i in range(len(cents_list)):\n",
    "        ens = os.listdir(f'/badc/cmip6/data/CMIP6/ScenarioMIP/{cents_list[i]}/{mods_list[i]}/ssp245/')\n",
    "        if len(ens) >= 1:\n",
    "            for e in range(len(ens)):\n",
    "                if 'r1i' in ens[e]:\n",
    "                    ens_list_s2.append(ens[e])\n",
    "                    break\n",
    "        elif len(ens) < 1:\n",
    "            print('THIS DOES NOT HAVE ENSEMBLE MEMBERS')\n",
    "else: print('CENTRES AND MODELS NOT SAME LENGTH')\n",
    "\n",
    "\n",
    "\n",
    "#list ensemble members\n",
    "#txt_list = []\n",
    "ens_list_hist = []\n",
    "if len(cents_list) == len(mods_list):\n",
    "    for i in range(len(cents_list)):\n",
    "        ens = os.listdir(f'/badc/cmip6/data/CMIP6/CMIP/{cents_list[i]}/{mods_list[i]}/historical/')\n",
    "        if ens_list_s2[i] in ens:\n",
    "            ens_list_hist.append(ens_list_s2[i])\n",
    "        else:\n",
    "            print(i)\n",
    "ens_list = ens_list_hist\n",
    "\n",
    "mods_list_pr, cents_list_pr, ens_list_pr = mods_list, cents_list, ens_list\n",
    "\n",
    "\n",
    "#actually retrieve them\n",
    "\n",
    "ds_list_s2 = []\n",
    "ds_list_hist = []\n",
    "\n",
    "dss = [ds_list_s2, ds_list_hist]\n",
    "scen = ['ssp245', 'historical']\n",
    "\n",
    "ds_long = []\n",
    "\n",
    "for x in range(2):\n",
    "    \n",
    "    for i in range(len(cents_list)):\n",
    "       # print(i)\n",
    "        path = f'/badc/cmip6/data/CMIP6/*MIP/{cents_list[i]}/{mods_list[i]}/{scen[x]}/{ens_list[i]}/Amon/pr/*/latest/'\n",
    "        ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
    "        dss[x].append(ds)\n",
    "    #    print(dss[x][i].institution_id)\n",
    "\n",
    "    #adding UKESM and CESM at the end \n",
    "    path = f'/badc/cmip6/data/CMIP6/*MIP/MOHC/UKESM1-0-LL/{scen[x]}/r10i1p1f2/Amon/pr/gn/latest/'\n",
    "    ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
    "    dss[x].append(ds)\n",
    "    \n",
    "    path = f'/badc/cmip6/data/CMIP6/*MIP/NCAR/CESM2-WACCM/{scen[x]}/r1i1p1f1/Amon/pr/gn/latest/'\n",
    "    ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
    "    dss[x].append(ds)\n",
    "\n",
    "\n",
    "for i in range(len(dss[0])):\n",
    "    DS_pr =  xr.concat([dss[1][i], dss[0][i]], dim='time')\n",
    "    ds_long.append(DS_pr)\n",
    "\n",
    "\n",
    "def get_TPC(da_pr):\n",
    "    \"\"\" \n",
    "    - Global returns 10yr avg of global precip for each lon-lat point\n",
    "    - Tropical returns 10yr avg of tropical precip for each lon-lat point \n",
    "    - TPC returns the latitude defining two regions of equal area-integrated \n",
    "    precipitation between two tropical latitude boundaries, 10 yr avg \"\"\"\n",
    "    \n",
    "    # global\n",
    "    pr_yr_glob = da_pr.groupby('time.year').mean('time')\n",
    "    pr_10_glob = pr_yr_glob.rolling(year=10, center=True).mean().dropna(\"year\") * 86400 # cause s to day \n",
    " #   pr_10_glob_mean = pr_10_glob.mean(dim = ['x', 'y'])\n",
    "\n",
    "    #tropical\n",
    "    da_pr_trop = da_pr.sel(y=slice(-20, 20))\n",
    "    pr_yr_trop = da_pr_trop.groupby('time.year').mean('time')\n",
    "    pr_10_trop = pr_yr_trop.rolling(year=10, center=True).mean().dropna(\"year\") * 86400 # cause s to day \n",
    "  #  pr_10_trop_mean = pr_10_trop.mean(dim = ['x','y'])\n",
    "\n",
    "    #centroid\n",
    "    weights = np.cos(np.deg2rad(da_pr.y))\n",
    "    da_pr_zonal = da_pr_trop.mean(dim='x')\n",
    "    da_pr_zonal_weighted = da_pr_zonal*weights\n",
    "    pr_weighted_lats = da_pr_trop.y.weighted(da_pr_zonal_weighted)\n",
    "    cr = pr_weighted_lats.mean(dim='y')\n",
    "    cr_yr = cr.groupby('time.year').mean('time')\n",
    "    cr_10 = cr_yr.rolling(year=10, center=True).mean().dropna(\"year\")\n",
    "    \n",
    "    return pr_10_glob, pr_10_trop, cr_10\n",
    "\n",
    "\n",
    "glob_pr = []\n",
    "trop_pr = []\n",
    "centroids = []\n",
    "for i in range(len(ds_long)):\n",
    "    glob, trop, cr = get_TPC(ds_long[i].pr)\n",
    "    glob_pr.append(glob)\n",
    "    trop_pr.append(trop)\n",
    "    centroids.append(cr)\n",
    "\n",
    "data_list = [glob_pr, trop_pr, centroids]\n",
    "names = ['glob_pr', 'trop_pr', 'centroids']\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \"inter_files_precip\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Save each DataArray in the list\n",
    "for data in range(len(data_list)):\n",
    "    dirpath = os.path.join(output_dir, names[data])\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    for i in range(len(data_list[data])):\n",
    "        filename = f\"{names[data]}_{i+1}.nc\"\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "    \n",
    "        # Save the DataArray to NetCDF format\n",
    "        data_list[data][i].to_netcdf(filepath)\n",
    "'''\n",
    "data_list = [patterns]\n",
    "names = ['patterns']\n",
    "\n",
    "output_dir = \"inter_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Save each DataArray in the list\n",
    "for data in range(len(data_list)):\n",
    "    dirpath = os.path.join(output_dir, names[data])\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    for i in range(len(data_list[data])):\n",
    "        filename = f\"{names[data]}_{i+1}.nc\"\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "    \n",
    "        # Save the DataArray to NetCDF format\n",
    "        data_list[data][i].to_netcdf(filepath)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88b7e1e1-d85d-481c-bd1d-6996037c6c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []\n",
    "\n",
    "for ds in range(len(ds_list_hist)):\n",
    "    ds_con = xr.concat([ds_list_hist[ds], ds_list_s2[ds]], dim = 'time')\n",
    "    ds_con_yrly = ds_con.groupby('time.year').mean('time')\n",
    "    pattern = ds_con_yrly.rolling(year=10, center=True).mean().dropna(\"year\")\n",
    "    patterns.append(pattern)\n",
    "    \n",
    "'''\n",
    "patterns_mean = []\n",
    "for i in patterns:\n",
    "    means = i.rolling(year=10, center=True).mean().dropna(\"year\")\n",
    "    patterns_mean.append(means)\n",
    "'''\n",
    "\n",
    "data_list = [patterns]\n",
    "names = ['patterns_precip']\n",
    "\n",
    "output_dir = \"inter_files_precip\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Save each DataArray in the list\n",
    "for data in range(len(data_list)):\n",
    "    dirpath = os.path.join(output_dir, names[data])\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    for i in range(len(data_list[data])):\n",
    "        filename = f\"{names[data]}_{i+1}.nc\"\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "    \n",
    "        # Save the DataArray to NetCDF format\n",
    "        data_list[data][i].to_netcdf(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00159bd0-9c93-4e1d-81b6-c7df4433ecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jaspy/lib/python3.11/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'tas' has multiple fill values {1e+20, 1e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/opt/jaspy/lib/python3.11/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'tas' has multiple fill values {1e+20, 1e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n",
      "/opt/jaspy/lib/python3.11/site-packages/xarray/conventions.py:286: SerializationWarning: variable 'tas' has multiple fill values {1e+20, 1e+20} defined, decoding all values to NaN.\n",
      "  var = coder.decode(var, name=name)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### TAS  \n",
    "\n",
    "\n",
    "#list centres for tas \n",
    "cents_list_s2 = os.listdir('/badc/cmip6/data/CMIP6/ScenarioMIP/')\n",
    "cents_list_s2.remove('DWD') #doesnt have ssp245, loser \n",
    "cents_list_s2.remove('HAMMOZ-Consortium') #doesnt have ssp245, loser \n",
    "cents_list_s2.remove('IPSL') # doesn't have Amon\n",
    "cents_list_s2.remove('UA') # dimensions are weird \n",
    "cents_list_s2.remove('CNRM-CERFACS') # s245 only runs until 2020\n",
    "cents_list_s2.remove('MOHC')#cause hardcoding these two for correct model\n",
    "cents_list_s2.remove('NCAR') #cause hardcoding these two for correct model\n",
    "#cents_list_s2.remove('NASA-GISS') # only has tas no precip\n",
    "cents_list_s2.remove('THU') # precip values are 1000x smaller than others\n",
    "cents_list_s2.remove('EC-Earth-Consortium') # starts 1975: no PI base \n",
    "\n",
    "#list centres - HIST\n",
    "cents_list_hist = os.listdir('/badc/cmip6/data/CMIP6/CMIP/')\n",
    "cents_list_hist.remove('UA') # dimensions are weird\n",
    "cents_list_hist.remove('IPSL') # doesn't have Amon\n",
    "cents_list_hist.remove('CNRM-CERFACS') # s245 only runs until 2020\n",
    "cents_list_hist.remove('MOHC')#cause hardcoding these two for correct model\n",
    "cents_list_hist.remove('NCAR') #cause hardcoding these two for correct model\n",
    "\n",
    "\n",
    "#list models(?)\n",
    "mods_list_s2 = []\n",
    "for c in cents_list_s2:\n",
    "    mods = os.listdir(f'/badc/cmip6/data/CMIP6/ScenarioMIP/{c}/')\n",
    "    if len(mods) >= 1:\n",
    "        mods_list_s2.append(mods[0])\n",
    "    elif len(mods) < 1:\n",
    "        print('THIS DOES NOT HAVE MODELS')\n",
    "\n",
    "#add the missing ones - surpassing loop:\n",
    "cents_list_s2.append('UA')\n",
    "mods_list_s2.append('IPSL-CM6A-LR')\n",
    "\n",
    "#list models(?)\n",
    "mods_list_hist = []\n",
    "for c in cents_list_hist:\n",
    "    mods = os.listdir(f'/badc/cmip6/data/CMIP6/CMIP/{c}/')\n",
    "    if len(mods) >= 1:\n",
    "        mods_list_hist.append(mods[0])\n",
    "    elif len(mods) < 1:\n",
    "        print('THIS DOES NOT HAVE MODELS')\n",
    "\n",
    "#add the missing ones - surpassing loop:\n",
    "#cents_list_hist.append('IPSL')\n",
    "#mods_list_hist.append('IPSL-CM6A-LR')\n",
    "\n",
    "mods_remove = []\n",
    "cents_remove = []\n",
    "\n",
    "for x in range(len(mods_list_s2)):\n",
    "    if not mods_list_s2[x] in mods_list_hist:\n",
    "        mods_remove.append(mods_list_s2[x])\n",
    "        cents_remove.append(cents_list_s2[x])\n",
    "for x in range(len(cents_remove)):\n",
    "    cents_list_s2.remove(cents_remove[x])\n",
    "    mods_list_s2.remove(mods_remove[x])\n",
    "    \n",
    "mods_remove = []\n",
    "cents_remove = []\n",
    "for x in range(len(mods_list_hist)):\n",
    "    if not mods_list_hist[x] in mods_list_s2:\n",
    "        mods_remove.append(mods_list_hist[x])\n",
    "        cents_remove.append(cents_list_hist[x])\n",
    "for x in range(len(cents_remove)):\n",
    "    cents_list_hist.remove(cents_remove[x])\n",
    "    mods_list_hist.remove(mods_remove[x])\n",
    "\n",
    "mods_list = []\n",
    "cents_list = []\n",
    "for x in range(len(mods_list_s2)):\n",
    "    if mods_list_s2[x] in mods_list_hist:\n",
    "        mods_list.append(mods_list_s2[x])\n",
    "        cents_list.append(cents_list_s2[x])\n",
    "\n",
    "#list ensemble members\n",
    "\n",
    "\n",
    "'''\n",
    "ens_list_s2 = []\n",
    "if len(cents_list) == len(mods_list):\n",
    "    for i in range(len(cents_list)):\n",
    "        ens = os.listdir(f'/badc/cmip6/data/CMIP6/ScenarioMIP/{cents_list[i]}/{mods_list[i]}/ssp245/')\n",
    "        if len(ens) >= 1:\n",
    "            for e in range(len(ens)):\n",
    "                if not '.' in ens[e]:\n",
    "                    ens_list_s2.append(ens[e])\n",
    "                    break\n",
    "        elif len(ens) < 1:\n",
    "            print('THIS DOES NOT HAVE ENSEMBLE MEMBERS')\n",
    "else: print('CENTRES AND MODELS NOT SAME LENGTH')\n",
    "'''\n",
    "# changing  if not '.' in ens[e]:   to    if 'r1i' in ens[e]:\n",
    "\n",
    "ens_list_s2 = []\n",
    "if len(cents_list) == len(mods_list):\n",
    "    for i in range(len(cents_list)):\n",
    "        ens = os.listdir(f'/badc/cmip6/data/CMIP6/ScenarioMIP/{cents_list[i]}/{mods_list[i]}/ssp245/')\n",
    "        if len(ens) >= 1:\n",
    "            for e in range(len(ens)):\n",
    "                if 'r1i' in ens[e]:\n",
    "                    ens_list_s2.append(ens[e])\n",
    "                    break\n",
    "        elif len(ens) < 1:\n",
    "            print('THIS DOES NOT HAVE ENSEMBLE MEMBERS')\n",
    "else: print('CENTRES AND MODELS NOT SAME LENGTH')\n",
    "\n",
    "    \n",
    "#list ensemble members\n",
    "\n",
    "ens_list_hist = []\n",
    "if len(cents_list) == len(mods_list):\n",
    "    for i in range(len(cents_list)):\n",
    "        ens = os.listdir(f'/badc/cmip6/data/CMIP6/CMIP/{cents_list[i]}/{mods_list[i]}/historical/')\n",
    "        if ens_list_s2[i] in ens:\n",
    "            ens_list_hist.append(ens_list_s2[i])\n",
    "        else:\n",
    "            print(i)\n",
    "ens_list = ens_list_hist\n",
    "\n",
    "mods_list_tas, cents_list_tas, ens_list_tas = mods_list, cents_list, ens_list\n",
    "\n",
    "\n",
    "ds_list_s2 = []\n",
    "ds_list_hist = []\n",
    "\n",
    "dss = [ds_list_s2, ds_list_hist]\n",
    "scen = ['ssp245', 'historical']\n",
    "\n",
    "#adding UKESM and CESM at the end \n",
    "for x in range(2):\n",
    "    \n",
    "    for i in range(len(cents_list)):\n",
    "       # print(i)\n",
    "        path = f'/badc/cmip6/data/CMIP6/*MIP/{cents_list[i]}/{mods_list[i]}/{scen[x]}/{ens_list[i]}/Amon/tas/*/latest/'\n",
    "        ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
    "        dss[x].append(ds)\n",
    "\n",
    "    path = f'/badc/cmip6/data/CMIP6/*MIP/MOHC/UKESM1-0-LL/{scen[x]}/r10i1p1f2/Amon/tas/gn/latest/'\n",
    "    ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
    "    dss[x].append(ds)\n",
    "    \n",
    "    path = f'/badc/cmip6/data/CMIP6/*MIP/NCAR/CESM2-WACCM/{scen[x]}/r1i1p1f1/Amon/tas/gn/latest/'\n",
    "    ds = rename_cmip6(xr.open_mfdataset(path+'*.nc'))\n",
    "    dss[x].append(ds)\n",
    "\n",
    "def get_ITD_multi(da_tas): \n",
    "    weights = np.cos(np.deg2rad(da_tas.y))\n",
    "    NH = da_tas.sel(y=slice(0, 90)).weighted(weights).mean(dim=['x', 'y'])\n",
    "    SH = da_tas.sel(y=slice(-90, 0)).weighted(weights).mean(dim=['x', 'y'])\n",
    "    ITD = NH-SH\n",
    "    NH = NH.groupby('time.year').mean('time')\n",
    "    SH = SH.groupby('time.year').mean('time')\n",
    "    ITD = ITD.groupby('time.year').mean('time')\n",
    "    GMT = (NH+SH)/2\n",
    "    pattern = da_tas.groupby('time.year').mean('time')\n",
    "    return ITD, NH,SH,GMT #, pattern\n",
    "\n",
    "\n",
    "ITDs = []\n",
    "NHs = []\n",
    "SHs = []\n",
    "GMTs = []\n",
    "#patterns = []\n",
    "\n",
    "for ds in range(len(ds_list_hist)):\n",
    "    ds_con = xr.concat([ds_list_hist[ds], ds_list_s2[ds]], dim = 'time')\n",
    "    itd, NH, SH, GMT = get_ITD_multi(ds_con.tas)\n",
    "    ITDs.append(itd)\n",
    "    NHs.append(NH)\n",
    "    SHs.append(SH)\n",
    "    GMTs.append(GMT)\n",
    " #   patterns.append(pattern)\n",
    "\n",
    "ITDs_mean = []\n",
    "for i in ITDs:\n",
    "    means = i.rolling(year=10, center=True).mean().dropna(\"year\")\n",
    "    ITDs_mean.append(means)\n",
    "\n",
    "NHs_mean = []\n",
    "for i in NHs:\n",
    "    means = i.rolling(year=10, center=True).mean().dropna(\"year\")\n",
    "    NHs_mean.append(means)\n",
    "\n",
    "SHs_mean = []\n",
    "for i in SHs:\n",
    "    means = i.rolling(year=10, center=True).mean().dropna(\"year\")\n",
    "    SHs_mean.append(means)\n",
    "\n",
    "GMTs_mean = []\n",
    "for i in GMTs:\n",
    "    means = i.rolling(year=10, center=True).mean().dropna(\"year\")\n",
    "    GMTs_mean.append(means)\n",
    "\n",
    "\n",
    "patterns = []\n",
    "\n",
    "for ds in range(len(ds_list_hist)):\n",
    "    ds_con = xr.concat([ds_list_hist[ds], ds_list_s2[ds]], dim = 'time')\n",
    "    ds_con_yrly = ds_con.groupby('time.year').mean('time')\n",
    "    pattern = ds_con_yrly.rolling(year=10, center=True).mean().dropna(\"year\")\n",
    "    patterns.append(pattern)\n",
    "    \n",
    "\n",
    "patterns_mean = []\n",
    "for i in patterns:\n",
    "    means = i.rolling(year=10, center=True).mean().dropna(\"year\")\n",
    "    patterns_mean.append(means)\n",
    "\n",
    "data_list = [ITDs_mean, NHs_mean, SHs_mean, GMTs_mean]\n",
    "names = ['ITDs', 'NHs', 'SHs', 'GMTs']\n",
    "\n",
    "# Define output directory\n",
    "output_dir = \"inter_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Save each DataArray in the list\n",
    "for data in range(len(data_list)):\n",
    "    dirpath = os.path.join(output_dir, names[data])\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    for i in range(len(data_list[data])):\n",
    "        filename = f\"{names[data]}_{i+1}.nc\"\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "    \n",
    "        # Save the DataArray to NetCDF format\n",
    "        data_list[data][i].to_netcdf(filepath)\n",
    "\n",
    "\n",
    "data_list = [patterns]\n",
    "names = ['patterns']\n",
    "\n",
    "output_dir = \"inter_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Save each DataArray in the list\n",
    "for data in range(len(data_list)):\n",
    "    dirpath = os.path.join(output_dir, names[data])\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    for i in range(len(data_list[data])):\n",
    "        filename = f\"{names[data]}_{i+1}.nc\"\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "    \n",
    "        # Save the DataArray to NetCDF format\n",
    "        data_list[data][i].to_netcdf(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461041c7-9e58-4262-a2e1-588a6756a5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmipv2",
   "language": "python",
   "name": "cmipv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
